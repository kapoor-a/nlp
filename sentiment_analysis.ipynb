{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNfPl5+JEd8GaAqWTucBfmy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kapoor-a/nlp/blob/main/sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ct819_TcFNKx"
      },
      "outputs": [],
      "source": [
        "!pip install trax\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import shutil\n",
        "import random as rnd\n",
        "\n",
        "# import relevant libraries\n",
        "import trax\n",
        "import trax.fastmath.numpy as np\n",
        "from trax import layers as tl\n",
        "from trax import fastmath"
      ],
      "metadata": {
        "id": "Q5QQ_apcFy25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords, twitter_samples \n",
        "from nltk.stem import PorterStemmer\n",
        "import string"
      ],
      "metadata": {
        "id": "ON7DwE4J3m6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_english = stopwords.words('english')\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "p2fDeK3Q4BY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_tweet(tweet):\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and # remove stopwords\n",
        "            word not in string.punctuation): # remove punctuation\n",
        "            #tweets_clean.append(word)\n",
        "            stem_word = stemmer.stem(word) # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "    return tweets_clean    "
      ],
      "metadata": {
        "id": "NrrfEQ3D4MoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_tweets():\n",
        "    all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "    all_negative_tweets = twitter_samples.strings('negative_tweets.json')  \n",
        "    return all_positive_tweets, all_negative_tweets"
      ],
      "metadata": {
        "id": "lAJIWFi74nea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_split():\n",
        "    all_positive_tweets, all_negative_tweets = load_tweets()\n",
        "\n",
        "    # View the total number of positive and negative tweets.\n",
        "    print(f\"The number of positive tweets: {len(all_positive_tweets)}\")\n",
        "    print(f\"The number of negative tweets: {len(all_negative_tweets)}\")\n",
        "    \n",
        "    split = int(0.9*len(all_positive_tweets))\n",
        "\n",
        "    val_pos   = all_positive_tweets[split:] \n",
        "    train_pos  = all_positive_tweets[:split]\n",
        "\n",
        "    val_neg   = all_negative_tweets[split:] \n",
        "    train_neg  = all_negative_tweets[:split]\n",
        "    \n",
        "    train_x = train_pos + train_neg \n",
        "\n",
        "    val_x  = val_pos + val_neg\n",
        "\n",
        "    # Set the labels for the training set (1 for positive, 0 for negative)\n",
        "    train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
        "\n",
        "    # Set the labels for the validation set (1 for positive, 0 for negative)\n",
        "    val_y  = np.append(np.ones(len(val_pos)), np.zeros(len(val_neg)))\n",
        "\n",
        "\n",
        "    return train_pos, train_neg, train_x, train_y, val_pos, val_neg, val_x, val_y"
      ],
      "metadata": {
        "id": "kLSKxGEK43cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocab(train_x):\n",
        "    # started with pad, end of line and unk tokens\n",
        "    vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2} \n",
        "\n",
        "    for tweet in train_x: \n",
        "        processed_tweet = process_tweet(tweet)\n",
        "        for word in processed_tweet:\n",
        "            if word not in vocab: \n",
        "                vocab[word] = len(vocab)\n",
        "    \n",
        "    return vocab"
      ],
      "metadata": {
        "id": "FRlwQUap5akz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tweet_to_tensor(tweet, vocab_dict, unk_token='__UNK__'):  \n",
        "    word_l = process_tweet(tweet)\n",
        "    tensor_l = [] \n",
        "    unk_id = vocab_dict[unk_token]\n",
        "    \n",
        "    for word in word_l:\n",
        "        word_id = vocab_dict[word] if word in vocab_dict else unk_id\n",
        "        tensor_l.append(word_id)\n",
        "    \n",
        "    return tensor_l"
      ],
      "metadata": {
        "id": "RM2VnPdn54Aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_generator(data_pos, data_neg, batch_size, vocab_dict, loop, shuffle=False):  \n",
        "    assert batch_size % 2 == 0\n",
        "    n_to_take = batch_size // 2\n",
        "    pos_index = 0\n",
        "    neg_index = 0\n",
        "    len_data_pos = len(data_pos)\n",
        "    len_data_neg = len(data_neg)\n",
        "    pos_index_lines = list(range(len_data_pos))\n",
        "    neg_index_lines = list(range(len_data_neg))\n",
        "    if shuffle:\n",
        "        rnd.shuffle(pos_index_lines)\n",
        "        rnd.shuffle(neg_index_lines)\n",
        "    targets = np.array([1]*n_to_take + [0]*n_to_take)\n",
        "    weights = np.array([1]*batch_size)    \n",
        "    stop = False\n",
        "    \n",
        "    while not stop:  \n",
        "        batch = []\n",
        "        for i in range(n_to_take):\n",
        "            if pos_index >= len_data_pos: \n",
        "                if not loop:\n",
        "                    stop = True;\n",
        "                    break;\n",
        "                pos_index = 0\n",
        "                if shuffle:\n",
        "                    rnd.shuffle(pos_index_lines)\n",
        "                    \n",
        "            tweet = data_pos[pos_index_lines[pos_index]]\n",
        "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
        "            batch.append(tensor)\n",
        "            pos_index = pos_index + 1\n",
        "\n",
        "        for i in range(n_to_take):\n",
        "            if neg_index >= len_data_neg:\n",
        "                if not loop:\n",
        "                    stop = True \n",
        "                    break \n",
        "                neg_index = 0\n",
        "                if shuffle:\n",
        "                    rnd.shuffle(neg_index_lines)\n",
        "            tweet = data_neg[neg_index_lines[neg_index]]\n",
        "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
        "            batch.append(tensor)\n",
        "            neg_index += 1\n",
        "\n",
        "        if stop:\n",
        "            break;\n",
        "\n",
        "        max_len = max([len(t) for t in batch]) \n",
        "        tensor_pad_l = []\n",
        "        for tensor in batch:\n",
        "            n_pad = max_len - len(tensor)\n",
        "            pad_l = [0]*n_pad\n",
        "            tensor_pad = tensor + pad_l\n",
        "            tensor_pad_l.append(tensor_pad)\n",
        "\n",
        "        inputs = np.array(tensor_pad_l)\n",
        "        yield inputs, targets, weights"
      ],
      "metadata": {
        "id": "D66-Av426KOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pos, train_neg, train_x, train_y, val_pos, val_neg, val_x, val_y = train_val_split()\n",
        "vocab = get_vocab(train_x)"
      ],
      "metadata": {
        "id": "7oqpYCxyFrlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_size = 256\n",
        "batch_size = 16"
      ],
      "metadata": {
        "id": "1X3K7ZoIHNkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_stream = data_generator(train_pos, train_neg, batch_size, vocab, loop=True, shuffle=True)\n",
        "eval_stream = data_generator(val_pos, val_neg, batch_size, vocab, True, True)"
      ],
      "metadata": {
        "id": "i7EolcoXIS1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(eval_stream)"
      ],
      "metadata": {
        "id": "6_pzG_7ROJxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classifier(vocab_size, embedding_dim, output_dim, mode='train'):\n",
        "    return tl.Serial( \n",
        "      tl.Embedding(vocab_size=vocab_size, d_feature=embedding_dim),\n",
        "      tl.Dense(embedding_dim),\n",
        "      tl.Mean(axis=1),\n",
        "      tl.Dense(output_dim),\n",
        "      tl.LogSoftmax()\n",
        "    ) "
      ],
      "metadata": {
        "id": "xO5gFZpsAXo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trax.supervised import training\n",
        "\n",
        "def training_loop(model, vocab, train_stream, eval_stream, output_dir=\"model/\"):\n",
        "    train_task = training.TrainTask(\n",
        "        labeled_data=train_stream,\n",
        "        loss_layer=tl.CrossEntropyLoss(),\n",
        "        optimizer=trax.optimizers.Adam(0.01),\n",
        "        n_steps_per_checkpoint=10,\n",
        "    )\n",
        "    eval_task = training.EvalTask(\n",
        "        labeled_data=eval_stream,\n",
        "        metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n",
        "    )\n",
        "\n",
        "    loop = training.Loop(model, tasks=train_task, eval_tasks=eval_task, output_dir=output_dir)\n",
        "    return loop"
      ],
      "metadata": {
        "id": "lalKaTKIBQxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/model/\n",
        "model = classifier(vocab_size, embedding_size, 2)\n",
        "loop = training_loop(model, vocab, train_stream, eval_stream, output_dir='/content/model/')"
      ],
      "metadata": {
        "id": "sx4Ac2IxJ29H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loop.run(n_steps = 500)"
      ],
      "metadata": {
        "id": "DSmM6RKyLWvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(sentence):\n",
        "    inputs = np.array(tweet_to_tensor(sentence, vocab_dict=vocab))\n",
        "    inputs = inputs[None, :]  \n",
        "    preds_probs = model(inputs)\n",
        "    print(preds_probs)\n",
        "    preds = int(preds_probs[0, 1] > preds_probs[0, 0])\n",
        "    sentiment = \"negative\"\n",
        "    if preds == 1:\n",
        "        sentiment = 'positive'\n",
        "\n",
        "    return preds, sentiment"
      ],
      "metadata": {
        "id": "5dIsYTwMPIng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(\"it feels bad when others are sad\")"
      ],
      "metadata": {
        "id": "9DTN6hE7Pqo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA #Import PCA from scikit-learn\n",
        "pca = PCA(n_components=2) #PCA with two dimensions\n",
        "\n",
        "emb_2dim = pca.fit_transform(model.weights[0])"
      ],
      "metadata": {
        "id": "vagCZhtuQH8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Selection of negative and positive words\n",
        "neg_words = ['worst', 'bad', 'hurt', 'sad', 'hate']\n",
        "pos_words = ['best', 'good', 'nice', 'better', 'love']\n",
        "\n",
        "#Index of each selected word\n",
        "neg_n = [vocab[w] for w in neg_words]\n",
        "pos_n = [vocab[w] for w in pos_words]\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "#Scatter plot for negative words\n",
        "plt.scatter(emb_2dim[neg_n][:,0],emb_2dim[neg_n][:,1], color = 'r')\n",
        "for i, txt in enumerate(neg_words): \n",
        "    plt.annotate(txt, (emb_2dim[neg_n][i,0],emb_2dim[neg_n][i,1]))\n",
        "\n",
        "#Scatter plot for positive words\n",
        "plt.scatter(emb_2dim[pos_n][:,0],emb_2dim[pos_n][:,1], color = 'g')\n",
        "for i, txt in enumerate(pos_words): \n",
        "    plt.annotate(txt,(emb_2dim[pos_n][i,0],emb_2dim[pos_n][i,1]))\n",
        "\n",
        "plt.title('Word embeddings in 2d')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vnILOtnWQa4V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}